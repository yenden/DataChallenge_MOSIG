{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=15%><img src=\"./img/UGA.png\"></img></td>\n",
    "<td><center><h1>Convex and Distributed Optimization</h1></center></td>\n",
    "<td width=15%><a href=\"http://www.iutzeler.org\" style=\"font-size: 16px; font-weight: bold\">Franck Iutzeler</a><br/>\n",
    "<a href=\"https://ljk.imag.fr/membres/Jerome.Malick/\" style=\"font-size: 16px; font-weight: bold\">Jérôme Malick</a><br/>\n",
    "<a href=\"https://tropars.github.io/\" style=\"font-size: 16px; font-weight: bold\">Thomas Ropars</a><br/>2017/2018 </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><div id=\"top\"></div>\n",
    "\n",
    "<center><a style=\"font-size: 40pt; font-weight: bold\">Lab. 1 - Incremental algorithms </a></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.notebook_setting as nbs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "packageList = ['IPython', 'numpy', 'matplotlib', 'pandas' , 'sklearn']\n",
    "nbs.packageCheck(packageList)\n",
    "\n",
    "nbs.cssStyling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><b>Outline</b><br/><br/>\n",
    "&nbsp;&nbsp;&nbsp; 1) <a href=\"#Classif\"> Classification </a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 2) <a href=\"#Student\"> the Student Performance Dataset</a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 3) <a href=\"#Optim\"> Logisitic loss optimization </a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 4) <a href=\"#Incremental\"> Incremental algorithms </a><br/>\n",
    "&nbsp;&nbsp;&nbsp; 5) <a href=\"#LS\"> Larger-scale experiments </a><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"><b>Warning:</b> This lab assumes basic knowledge about Python and basic machine learning libraries (numpy, scikit-learn, pandas). If you are not familiar with those, check out <a href=\"https://github.com/iutzeler/Introduction-to-Python-for-Data-Sciences\">this introduction</a>. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Classif\"> 1) Classification</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of classfication if the one of finding rules for assigning a class to a given vector from already classified data, for instance, the 2D points below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib inline\n",
    "\n",
    "# we create 40 separable points in R^2 around 2 centers\n",
    "X, y = make_blobs(n_samples=40, n_features=2, centers=2 , random_state=48443)\n",
    "\n",
    "print(X[:5,:],y[:5]) # print the first 5 points and labels\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are based on learning a vector $w$ and an intercept $b$ such that the hyperplane $w^T x - b = 0$ separates the data i.e.  $a$ belongs to one class if  $w^T a - b > 0$ and the other elsewhere. \n",
    "\n",
    "\n",
    "The ``scikit-learn`` library provides a classification module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # Support vector classifier i.e. Classifier by SVM\n",
    "\n",
    "modelSVMLinear = SVC(kernel=\"linear\")\n",
    "modelSVMLinear.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustration can be found in the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',levels=[ 0], alpha=0.5,    linestyles=[ '--'])\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y ,  cmap=plt.cm.Paired)\n",
    "plot_svc_decision_function(modelSVMLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see clearly that the linear SVM seeks at maximizing the *margin* between the hyperplane and the two well defined classes from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <a id=\"Student\"> 2) the Student Performance Dataset </a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the <a href=\"http://archive.ics.uci.edu/ml/datasets/Student+Performance\">student performance</a> dataset. The goal is to predict if the student will pass (i.e. the final grade is greater than 12) from the other information, we get from the documentation:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Attributes for both student-mat.csv (Math course) dataset:\n",
    "1 sex - student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "2 age - student's age (numeric: from 15 to 22)\n",
    "3 address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "4 famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "5 Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "6 Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "7 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "8 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "9 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "10 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "11 schoolsup - extra educational support (binary: yes or no)\n",
    "12 famsup - family educational support (binary: yes or no)\n",
    "13 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "14 activities - extra-curricular activities (binary: yes or no)\n",
    "15 nursery - attended nursery school (binary: yes or no)\n",
    "16 higher - wants to take higher education (binary: yes or no)\n",
    "17 internet - Internet access at home (binary: yes or no)\n",
    "18 romantic - with a romantic relationship (binary: yes or no)\n",
    "19 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "20 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "21 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "22 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "23 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "24 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "25 absences - number of school absences (numeric: from 0 to 93)\n",
    "26 G1 - first period grade (numeric: from 0 to 20)\n",
    "27 G2 - second period grade (numeric: from 0 to 20)\n",
    "\n",
    "28 G3 - final grade (numeric: from 0 to 20, output target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows for easy reading and provides a dataframe of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "student = pd.read_csv('data/student-mat.csv')\n",
    "student.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = pd.DataFrame(student[\"G3\"])\n",
    "features = student.drop([\"G3\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice DataFrame operations, we first look at the values of the target and features and investigate basic transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Use Pandas's <tt>applymap</tt> function to transform the target from numerical notes to +1 (pass, note >= 12) and -1 (fail, note < 12).<br/>\n",
    "<i>Hint: create a function return +1 if the input is >= 12 and -1 elsewhere first and apply this function using <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html\">applymap</a>.</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(x):\n",
    "    if x>=12:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "target_classes = target.applymap(to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One immediate problem here is that the features are not *numeric* (not floats). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Use Pandas's <tt>groupby</tt> function to count the number of boys and girls, and the number of people without Internet access. Finally, use <tt>describe</tt> to get age statistics per sex. </i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.groupby(\"sex\")[\"age\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.groupby(\"internet\")[\"age\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.groupby(\"sex\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features are not supported natively by optimizers; thankfully, Scikit Learn provides [encoders](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) to convert categorical (aka nominal, discrete) features to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lenc = LabelEncoder()\n",
    "num_features = features.apply(lenc.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even numerical values were encoded, as we are going to normalize, it is not really important. \n",
    "\n",
    "The normalization is done by removing the mean and equalizing the variance per feature, in addition, we are going to add an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, add_dummy_feature\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normFeatures = add_dummy_feature(scaler.fit_transform(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preproData = pd.DataFrame(normFeatures , columns=[ \"intercept\" ] + list(num_features.columns) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproData.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to split the dataset into learning and training sets with <tt>train_test_split</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split    # sklearn > ...\n",
    "except:\n",
    "    from sklearn.cross_validation import train_test_split   # sklearn < ...\n",
    "    \n",
    "XTrain, XTest, yTrain, yTest = train_test_split(preproData,target_classes,test_size = 0.25) # split data in two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us solve this classification problem using scikit learn's routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Use Scikit-Learn to perform a first classification and evaluate the obtained performance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # Support vector classifier i.e. Classifier by SVM\n",
    "\n",
    "modelSVMLinear = SVC(kernel=\"linear\")\n",
    "modelSVMLinear.fit(XTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "yPred = modelSVMLinear.predict(XTest)\n",
    "mat = confusion_matrix(yTest, yPred)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True ,cbar=False)\n",
    "plt.ylabel('true label')\n",
    "plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warn\"><b>Warning:</b> Scikit-learn provides efficient classification routines for up to medium scale datasets. However, to treat larger datasets, it is fundamental to understand the optimization problems and minimization methods.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <a id=\"Optim\"> 3) Logistic loss Optimization</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the optimization part of the lab.\n",
    "\n",
    "In our classification setup, the observations are binary in $\\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda}{2} \\|x\\|_2^2 .\n",
    "\\end{align*}\n",
    "where the last term is added as a regularization (of type $\\ell_2$, aka Tikhnov) to prevent overfitting. This function is **$L$-smooth and $\\lambda$ strongly-convex.**\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[a\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = 0.001 #  REGULARIZATION \n",
    "\n",
    "m,n = XTrain.shape # SIZES: m = Number of examples n = feature size \n",
    "\n",
    "L = 0.25*max(np.linalg.norm(preproData.values,2,axis=1))**2 + lam # Lispchitz constant of f\n",
    "mu = lam # Strong convexity constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below find the oracles functions for f and its gradient (taken from the resfresher course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    A = XTrain.values\n",
    "    b = yTrain.values\n",
    "    l = 0.0\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0 :\n",
    "            l += np.log( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            l += np.log( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "    return l/m + lam/2.0*np.dot(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_grad(x):\n",
    "    A = XTrain.values\n",
    "    b = yTrain.values\n",
    "    g = np.zeros(n)\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0:\n",
    "            g += -A[i]/( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            g += A[i]/( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "    return g/m + lam*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Implement a gradient descent algorithm on f and display its results in terms of functional decrease on the training set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    stop = PREC*np.linalg.norm(f_grad(x0) )\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n Constant Stepsize gradient\\n------------------------------------\\nSTART    -- stepsize = {:0}\".format(step))\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        g = f_grad(x)\n",
    "        x = x - step*g  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "        if np.linalg.norm(g) < stop:\n",
    "            break\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm \n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(n)              # Initial point\n",
    "step    = 1.99/(L)\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(f , f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F =  []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append(f(x_tab[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F, color=\"black\", linewidth=1.0, linestyle=\"-\", label=\"gradient algorithm\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Plot the confusion matrix on the testing set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPredLog = np.sign( 1/(1.0 + np.exp( - (XTest.values).dot(x) )) - 0.5) # +1 if Prob (a) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(yTest, yPredLog)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True ,cbar=False);\n",
    "plt.ylabel('true label');\n",
    "plt.xlabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"Incremental\"> 4) Incremental Algorithms</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m \\underbrace{ \\left(  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda}{2m} \\|x\\|_2^2 \\right)}_{f_i(x)}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Define the oracle for the gradient based on only one training example $\\nabla f_i(x)$.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_grad_ex(i,x):\n",
    "    A = XTrain.values[i,:]\n",
    "    b = yTrain.values[i]\n",
    "    g = np.zeros(n)\n",
    "    if b > 0:\n",
    "        g += -A/( 1 + np.exp(np.dot( A , x ) ) ) \n",
    "    else:\n",
    "        g += A/( 1 + np.exp(-np.dot( A , x ) ) ) \n",
    "    return g + lam*x/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Implement a stochastic gradient descent algorithm.<br/>\n",
    "<i>Hint: use <tt>np.random.choice</tt> to draw an example.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def stochastic_gradient_algorithm(f , f_grad_ex , x0  , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "\n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n Stochastic gradient\\n------------------------------------\\nSTART \")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        \n",
    "        i = np.random.choice(m)\n",
    "        step = 1.0/(L*(k+1)**0.7)\n",
    "        \n",
    "        g = f_grad_ex(i,x)\n",
    "        x = x - step*g  #######  ITERATION\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm \n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.zeros(n)              # Initial point\n",
    "\n",
    "##### gradient algorithm\n",
    "x_sto,x_sto_tab = stochastic_gradient_algorithm(f , f_grad_ex , x0   , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_sto =  []\n",
    "Ite_sto = []\n",
    "for i in range(0,x_sto_tab.shape[0],int(x_sto_tab.shape[0]/500.0)):\n",
    "    F_sto.append(f(x_sto_tab[i,:]))\n",
    "    Ite_sto.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ite_sto, F_sto, color=\"black\", linewidth=1.0, linestyle=\"-\", label=\"stochastic gradient algorithm\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\", label=\"gradient algorithm\");\n",
    "plt.plot([x/float(m) for x in Ite_sto], F_sto, color=\"red\", linewidth=1.0, linestyle=\"-\", label=\"stochastic gradient algorithm\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Implement SAGA and SVRG algorithms on the previous problem and compare with the gradient in terms of loss vs. number of passes over the data and time.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def SAGA(f , f_grad_ex , x0 , step  , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    p_tab = np.zeros((m,x0.size))\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n SAGA\\n------------------------------------\\nSTART \")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(ITE_MAX):\n",
    "        \n",
    "        e = np.random.choice(m)\n",
    "                \n",
    "        g = f_grad_ex(e,x)\n",
    "        \n",
    "        x = x - step*(g - p_tab[e,:] + sum(p_tab)/float(m) )\n",
    "        \n",
    "        p_tab[e,:] = np.copy(g)\n",
    "\n",
    "        x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm \n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.zeros(n)              # Initial point\n",
    "step    = 1/(2*(mu*m + L))\n",
    "\n",
    "##### gradient algorithm\n",
    "x_saga,x_saga_tab = SAGA(f , f_grad_ex , x0 , step  , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_saga =  []\n",
    "Ite_saga = []\n",
    "for i in range(0,x_saga_tab.shape[0],int(x_saga_tab.shape[0]/500.0)):\n",
    "    F_saga.append(f(x_saga_tab[i,:]))\n",
    "    Ite_saga.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def SVRG(f ,  f_grad_ex , x0 , step  , ITE_MAX ):\n",
    "    x = np.copy(x0)\n",
    "    p_tab = np.zeros((m,x0.size))\n",
    "    \n",
    "    M = 20\n",
    "    \n",
    "    x_tab = np.copy(x)\n",
    "    print(\"------------------------------------\\n SVRG\\n------------------------------------\\nSTART \")\n",
    "    t_s =  timeit.default_timer()\n",
    "    for k in range(int(ITE_MAX/M)):\n",
    "        \n",
    "        for i in range(m):\n",
    "            p_tab[i,:] = f_grad_ex(i,x)\n",
    "        \n",
    "        true_grad = sum(p_tab)/float(m)\n",
    "        \n",
    "        for j in range(M):\n",
    "            e = np.random.choice(m)\n",
    "              \n",
    "            g = f_grad_ex(e,x)\n",
    "        \n",
    "            x = x - step*(g - p_tab[e,:] +true_grad )\n",
    "        \n",
    "            x_tab = np.vstack((x_tab,x))\n",
    "\n",
    "    t_e =  timeit.default_timer()\n",
    "    print(\"FINISHED -- {:d} iterations / {:.6f}s -- final value: {:f}\\n\\n\".format(k,t_e-t_s,f(x)))\n",
    "    return x,x_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm \n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.zeros(n)              # Initial point\n",
    "step    = 0.1/( L)\n",
    "\n",
    "##### gradient algorithm\n",
    "x_svrg,x_svrg_tab = SVRG(f , f_grad_ex , x0 , step  , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_svrg =  []\n",
    "Ite_svrg = []\n",
    "for i in range(0,x_svrg_tab.shape[0],int(x_svrg_tab.shape[0]/500.0)):\n",
    "    F_svrg.append(f(x_svrg_tab[i,:]))\n",
    "    Ite_svrg.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\", label=\"gradient algorithm\");\n",
    "plt.plot([x/float(m) for x in Ite_sto], F_sto, color=\"red\", linewidth=1.0, linestyle=\"-\", label=\"stochastic gradient algorithm\");\n",
    "plt.plot([x/float(m) for x in Ite_saga], F_saga, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"SAGA\");\n",
    "plt.plot([x/float(m) for x in Ite_svrg], F_svrg, color=\"green\", linewidth=1.0, linestyle=\"-\", label=\"SVRG\");\n",
    "plt.xlim([0,70])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"LS\"> 5) Larger-Scale experiments</a>  \n",
    "\n",
    "\n",
    "<p style=\"text-align: right; font-size: 10px;\"><a href=\"#top\">Go to top</a></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``covtype`` dataset is slightly bigger: 581012 examples with 54 features. It represents forest cover type from cartographic variables only (no remotely sensed data) on 30 x 30 meter cell as determined by the US Forest Service (USFS). The features are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Elevation / quantitative /meters / Elevation in meters\n",
    "Aspect / quantitative / azimuth / Aspect in degrees azimuth\n",
    "Slope / quantitative / degrees / Slope in degrees\n",
    "Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features\n",
    "Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features\n",
    "Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway\n",
    "Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice\n",
    "Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice\n",
    "Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice\n",
    "Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points\n",
    "Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\n",
    "Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation\n",
    "Cover_Type or 1 or 2 / Forest Cover Type designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "X, y = load_svmlight_file(\"data/covtype.libsvm.binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, add_dummy_feature\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(X.todense())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normFeatures = add_dummy_feature(scaler.fit_transform(X))\n",
    "\n",
    "y = np.sign(y-1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUTOFF = 1000 # We limit ourselves to 10000 samples\n",
    "m = CUTOFF\n",
    "X = X[:CUTOFF,:]\n",
    "y = y[:CUTOFF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split    # sklearn > ...\n",
    "except:\n",
    "    from sklearn.cross_validation import train_test_split   # sklearn < ...\n",
    "    \n",
    "XCovTrain, XCovTest, yCovTrain, yCovTest = train_test_split(X,y,test_size = 0.5) # split data in two\n",
    "\n",
    "m,n = XCovTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exo\"> <b>Question:</b> Compare SGD and SAGA on this dataset.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lam = 1.0/m\n",
    "L = 0.25*max(np.linalg.norm(XCovTrain,2,axis=1))**2 + lam # Lispchitz constant of f\n",
    "mu = lam # Strong convexity constant\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    A = XCovTrain\n",
    "    b = yCovTrain\n",
    "    l = 0.0\n",
    "    for i in range(A.shape[0]):\n",
    "        if b[i] > 0 :\n",
    "            l += np.log( 1 + np.exp(-np.dot( A[i] , x ) ) ) \n",
    "        else:\n",
    "            l += np.log( 1 + np.exp(np.dot( A[i] , x ) ) ) \n",
    "    return l/m + lam/2.0*np.dot(x,x)\n",
    "\n",
    "\n",
    "def f_grad_ex(i,x):\n",
    "    A = XCovTrain[i,:]\n",
    "    b = yCovTrain[i]\n",
    "    g = np.zeros(n)\n",
    "    if b > 0:\n",
    "        g +=  -A/( 1 + np.exp(np.dot( A , x ) ) ) \n",
    "    else:\n",
    "        g +=  A/( 1 + np.exp(-np.dot( A , x ) ) ) \n",
    "    return g + lam*x/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm \n",
    "ITE_MAX = 10000                     # Max number of iterations\n",
    "x0      = np.zeros(n)              # Initial point\n",
    "step    = 1.0/(2*(mu*m + L))\n",
    "\n",
    "##### gradient algorithm\n",
    "x_sagaC,x_sagaC_tab = SAGA(f , f_grad_ex , x0 , step  , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_sagaC =  []\n",
    "Ite_sagaC = []\n",
    "for i in range(0,x_sagaC_tab.shape[0],int(x_sagaC_tab.shape[0]/500.0)+1):\n",
    "    F_sagaC.append(f(x_sagaC_tab[i,:]))\n",
    "    Ite_sagaC.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot([x/float(m) for x in Ite_sagaC], F_sagaC, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"SAGA\");\n",
    "#plt.xlim([0,1])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPredCov = np.sign( 1/(1.0 + np.exp( - XCovTest.dot(x_sagaC) )) - 0.5) # +1 if Prob (a) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "mat = confusion_matrix(yCovTest, yPredCov)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True ,cbar=False);\n",
    "plt.ylabel('true label');\n",
    "plt.xlabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " max( 1/(1.0 + np.exp( - XCovTest.dot(x_sagaC) )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "462px",
    "left": "1160px",
    "right": "47px",
    "top": "174px",
    "width": "553px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
